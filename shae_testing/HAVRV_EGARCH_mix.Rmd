---
title: "SVR_EGARCH_mix"
output: html_document
date: "2024-04-28"
---

```{r}
library(dplyr)
library(rugarch)
```


# Create a list of all of the stock file names
```{r}
dir_stocks <- "~/Documents/DATA3888/optiver_volatility/data/individual_book_train/"
all_stocks <- list.files(dir_stocks)
```

# Select 5 stocks at random
# THIS SHOULD BE CHANGED TO ENSURE CLASS BALANCE BETWEEN 4 CLUSTERS
```{r}

set.seed(23542355)

four_stocks <- sample(all_stocks, 4)

stock_files_list <- list()
for (i in four_stocks) {
  stock_files_list[[i]] <- read.csv(file.path(dir_stocks, i))
}

```


# Produce Num Order, WAP and Bid Ask Spread
```{r}

for (i in 1 : length(stock_files_list)) {
  stock_files_list[[i]] <- stock_files_list[[i]] %>% 
    mutate(WAP = (bid_price1 * ask_size1 + ask_price1 * bid_size1) / (bid_size1 + ask_size1)) %>%
    mutate(BidAskSpread = ask_price1 / bid_price1 - 1) %>%
    mutate(num_order = bid_size1 + ask_size1 + bid_size2 + ask_size2) %>%
    mutate(imbalance = (bid_size1 - ask_size1) / (bid_size1 + ask_size1)) %>%
    mutate(volume = (ask_size1 + bid_size1)) %>%
    mutate(range = (ask_price1 - bid_price1))
}

```


# Strip each stock file of the 500 unique time buckets - 2000 time minute buckets
```{r results = 'hide'}

bucket_list <- list()
for (j in 1 : length(stock_files_list)) {
  time_IDs <- unique(stock_files_list[[j]][, 1])[1:500]
  for (i in 1 : length(time_IDs)) {
    bucket <- stock_files_list[[j]] %>% filter(time_id == time_IDs[i])
    bucket_list[[length(bucket_list) + 1]] <- bucket
  }
}

bucket_list[[2000]]

```

# classify each of these buckets into one of the 4 clusters
# this produces a nested list of time buckets for each cluster
```{r results = 'hide'}

bucket_bas <- list()
for (i in 1 : length(bucket_list)) {
  bas <- bucket_list[[i]] %>% pull(BidAskSpread, imbalance)
  bucket_bas[[i]] <- mean(bas)
}

km.out <- kmeans(bucket_bas, centers = 4, nstart = 20)


cluster_data_lists <- list()
clusters <- list()
for (i in 1:length(bucket_list)) {
  cluster <- km.out$cluster[[i]]

  if (!(cluster %in% clusters)) {
    cluster_data_lists[[cluster]] <- list()
  }
  clusters <- c(clusters, cluster)
  
  cluster_data_lists[[cluster]][[length(cluster_data_lists[[cluster]]) + 1]] <- bucket_list[[i]]
}

length(cluster_data_lists[[1]])

```

# Stratifying into clusters
```{r results = 'hide'}
# c1 = do.call(rbind, cluster_data_lists[[1]])
# c2 = do.call(rbind, cluster_data_lists[[2]])
# c3 = do.call(rbind, cluster_data_lists[[3]])
# c4 = do.call(rbind, cluster_data_lists[[4]])
```


# Building functions for cluster log returns and volatility calculations
# This is far less efficient that nested for loops on indexes idk why
```{r results = 'hide'}

# Compute volatility
# comp_vol <- function(x) {
#   return(sqrt(sum(x ^ 2)))
# }

# Compute cluster log returns
# Takes in a time bucket data frame as input
# SOMETHING IS WRONG HERE - DO NOT USE
# comp_cluster_logr <- function(x) {
#   stock_ids <- unique(x %>% pull(stock_id)) 
#   
#   for (stock_id in stock_ids) {
#     x_filter <- x %>% filter(stock_id == stock_id)
#     unique_time_IDs <- unique(x_filter[, 1])
# 
#     for (time_id in unique_time_IDs) {
#       if (time_id %% 10 == 0) {
#         print(time_id)
#       }
#       sec <- x_filter %>% filter(time_id == time_id) %>% pull(seconds_in_bucket)
#       price <- x_filter %>% filter(time_id == time_id) %>% pull(WAP)
#       log_r <- log(price[-1] / price[1:(length(price) - 1)])
#       log_r1[[length(log_r1) + 1]] <- data.frame(time = sec[-1], log_return = log_r)
#       time.no.change <- (1:600)[!(1:600 %in% log_r1[[length(log_r1)]]$time)]
#       if (length(time.no.change) > 0) {
#         new.df <- data.frame(time = time.no.change, log_return = 0)
#         log_r1[[length(log_r1)]] <- rbind(log_r1[[length(log_r1)]], new.df)
#         log_r1[[length(log_r1)]] <- log_r1[[length(log_r1)]][order(log_r1[[length(log_r1)]]$time), ]
#       }
#     }
#   }
#   
#   return(log_r1)
# }

# # Compute cluster volatility
# # Takes in log returns data frame as input
# comp_cluster_vol <- function(x) {
#   for (i in 1 : length(x)) {
#     x[[i]] <- x[[i]] %>% mutate(time_bucket = ifelse(time == 0,1,ceiling(time / 30)))
#     vol[[i]] <- aggregate(log_return ~ time_bucket, data = x[[i]], FUN = comp_vol)
#     colnames(vol[[i]]) <- c('time_bucket', 'volatility')
#   }
#   return(vol)
# }
# 
# log_r1 <- list()
# log_r2 <- list()
# log_r3 <- list()
# log_r4 <- list()
# 
# log_r1[[23]] = comp_cluster_logr(c1)
# log_r2 = comp_cluster_logr(c2)
# log_r3 = comp_cluster_logr(c3)
# log_r4 = comp_cluster_logr(c4)
# 
# vol1 <- list()
# vol2 <- list()
# vol3 <- list()
# vol4 <- list()
# 
# vol1 = comp_cluster_vol(log_r1)
# vol2 = comp_cluster_vol(log_r2)
# vol3 = comp_cluster_vol(log_r3)
# vol4 = comp_cluster_vol(log_r4)

```


# Deriving the log returns for each time bucket
```{r results = 'hide'}
# Multiple time bucket approch - on all clusters

cluster_log_r1 <- list()
for (j in 1 : 4) {
  log_r1 <- list()
  for (i in 1 : length(cluster_data_lists[[j]])) {
    sec <- cluster_data_lists[[j]][[i]] %>% pull(seconds_in_bucket)
    price <- cluster_data_lists[[j]][[i]] %>% pull(WAP)
    log_r <- log(price[-1] / price[1:(length(price) - 1)])
    log_r1[[i]] <- data.frame(time = sec[-1], log_return = log_r)
    time.no.change <- (1:600)[!(1:600 %in% log_r1[[i]]$time)]
    if (length(time.no.change) > 0) {
      new.df <- data.frame(time = time.no.change, log_return = 0)
      log_r1[[i]] <- rbind(log_r1[[i]], new.df)
      log_r1[[i]] <- log_r1[[i]][order(log_r1[[i]]$time), ]
    }
  } 
  cluster_log_r1[[j]] <- log_r1
}

cluster_log_r1[[4]][[1]]

```


# Deriving the volatility for each time
```{r results = 'hide'}

# Multiple cluster approach 

comp_vol <- function(x) {
  return(sqrt(sum(x ^ 2)))
}

cluster_vol <- list()
for (j in 1:4) {
  vol <- list()
  for (i in 1 : length(cluster_log_r1[[j]])) {
  cluster_log_r1[[j]][[i]] <- cluster_log_r1[[j]][[i]] %>% mutate(time_bucket = ceiling(time / 30)) # I DON'T KNOW ABOUT THIS
  vol[[i]] <- aggregate(log_return ~ time_bucket, data = cluster_log_r1[[j]][[i]], FUN = comp_vol)
  colnames(vol[[i]]) <- c('time_bucket', 'volatility')
  }
  cluster_vol[[j]] <- vol
}

```


# EGARCH Development
```{r results = 'hide'}
spec <- ugarchspec(variance.model = list(model = "eGARCH", garchOrder = c(1, 1)), 
                   mean.model = list(armaOrder = c(1, 1)), 
                   distribution.model = "norm")

cluster_GARCH_models <- list()
for (j in 1:4) {
  ARMA_GARCH.models <- list()
  for (i in 1 : length(cluster_vol[[j]])) {
    ARMA_GARCH.models[[i]] <- ugarchfit(spec = spec, 
                                        data = cluster_log_r1[[j]][[i]] %>% 
                                               filter(time <= 450) %>%
                                               pull(log_return),
                                        solver = 'hybrid')
  } 
  cluster_GARCH_models[[j]] <- ARMA_GARCH.models
}

length(as.list(coef(cluster_GARCH_models[[4]][[103]])))

```

```{r results = 'hide'}

# Application on multiple clusters
cluster_rv_pred <- list()
for (j in 1:4) {
  RV.pred <- rep(list(), length(cluster_vol[[j]]))
  
  for (i in 1 : length(cluster_vol[[j]])) {
    fspec <- getspec(cluster_GARCH_models[[j]][[i]])
    if (length(as.list(coef(cluster_GARCH_models[[j]][[i]]))) == 0) { # HAS NAs CAUSING PROBLEMS 
      next
    }
    setfixed(fspec) <- as.list(coef(cluster_GARCH_models[[j]][[i]]))
    future.path <- fitted(ugarchpath(fspec, n.sim = 150, m.sim = 1000))
    future.path[is.na(future.path)] <- 0 
    
    interval_length <- 30
    num_intervals <- 5
    interval_volatility <- numeric(num_intervals)
    
    for (k in 1:num_intervals) {
      start_index <- (k - 1) * interval_length + 1
      end_index <- k * interval_length
      
      interval_volatility[k] <- mean(sqrt(colSums(future.path[start_index:end_index, ]^2)))
    }
    
    RV.pred[[i]] <- interval_volatility
  }
  
  cluster_rv_pred[[j]] <- RV.pred
}

plot(cluster_rv_pred[[4]][[77]])

```

# Cluster Evaluation Test Set
```{r results = 'hide'}

# Multiple cluster application

cluster_vol_train <- list()
cluster_vol_val <- list()
for (j in 1:4) {
  vol.train <- list()
  vol.val <- list()
  
  for (i in 1 : length(cluster_log_r1[[j]])) {
    vol.train[[i]] <- cluster_vol[[j]][[i]][1:15, ]
    vol.val[[i]] <- cluster_vol[[j]][[i]][-(1:15), ]
  }
  cluster_vol_train[[j]] <- vol.train
  cluster_vol_val[[j]] <- vol.val
}


```


# eGARCH MSE and QLIKE

```{r warning = FALSE}
par(mfrow = c(2, 2))

all_MSE <- list()
all_QLIKE <- list()

for (j in 1:4) {
  MSE.lm <- vector()
  QLIKE.lm <- vector()
  
  for (i in 1:length(cluster_vol[[j]])) {
    MSE.lm <- c(MSE.lm, mean((cluster_vol_val[[j]][[i]]$volatility - cluster_rv_pred[[j]][[i]]) ^ 2))
    QLIKE.lm <- c(QLIKE.lm, mean(cluster_vol_val[[j]][[i]]$volatility / cluster_rv_pred[[j]][[i]] - 
                                   log(cluster_vol_val[[j]][[i]]$volatility / cluster_rv_pred[[j]][[i]]) - 1))
  }
  
  # I don't know how necessary this is
  # max_index <- which.max(MSE.lm)
  # MSE.lm_filtered <- MSE.lm[-max_index]
  
  all_MSE[[j]] <- MSE.lm
  all_QLIKE[[j]] <- QLIKE.lm
  
}

for (j in 1:4) {
  boxplot(all_MSE[[j]], horizontal = TRUE, main = paste("MSE Plot for cluster", j), ylim = c(0, 0.00003))
}

for (j in 1:4) {
  boxplot(all_QLIKE[[j]], horizontal = TRUE, main = paste("QLIKE Plot for cluster", j), ylim = c(0, 3))
}

```

# HAV-RV Model development
```{r results = 'hide'}
cluster_vol.train <- list()
cluster_vol.val <- list()

for (j in 1:4) { 
  vol.train <- list()
  vol.val <- list()
  
  for (i in 1:length(cluster_log_r1[[j]])) {
    vol.train[[i]] <- cluster_vol[[j]][[i]][1:15, ]  # First 15 Time buckets for Training
    vol.val[[i]] <- cluster_vol[[j]][[i]][16:20, ]  # Remaining 5 Time buckets for validation
  }
  
  cluster_vol.train[[j]] <- vol.train
  cluster_vol.val[[j]] <- vol.val
}

cluster_vol.train[[3]][[10]]
cluster_vol.val[[2]][[10]]
```

# This is going to derive volatility, volatility at past time period and the mean voltaility over the past 5
```{r results = 'hide'}
list_HAV_cluster <- list()

for (j in 1:4) {
  list_HAV <- list()
  for (i in 1:length(cluster_vol[[j]])) {
    len.train <- length(cluster_vol.train[[j]][[i]]$volatility)
    mean.vol <- rep(0, len.train - 5)
    
    for (k in 1:5) {
      mean.vol <- mean.vol + cluster_vol.train[[j]][[i]]$volatility[k:(k + len.train - 6)] / 5
    }
    
    list_HAV[[i]] <- data.frame(
      vol = cluster_vol.train[[j]][[i]]$volatility[-(1:5)], 
      vol_1 = cluster_vol.train[[j]][[i]]$volatility[5:(len.train - 1)],
      mean_vol_5 = mean.vol
    )
  }
  list_HAV_cluster[[j]] <- list_HAV
}

list_HAV_cluster[[3]][[10]]
cluster_vol.train[[3]][[10]]

```


```{r results = 'hide'}
cluster_quar <- list() 
comp_quar <- function(x) {
  return(length(x) / 3 * sum(x ^ 4))
}

for (j in 1:4) {  
  quar <- list()
  for (i in 1:length(cluster_log_r1[[j]])) {
    quar[[i]] <- aggregate(log_return ~ time_bucket, data = cluster_log_r1[[j]][[i]], FUN = comp_quar)
    colnames(quar[[i]]) <- c('time_bucket', 'quarticity')
  }
  cluster_quar[[j]] <- quar
}

cluster_quar[[3]][[10]]
cluster_quar[[2]][[10]]
```

# Is there a reason we're only using WLS? Is this an intentional decision? 
```{r results = 'hide'}
list_HAV_wls_cluster <- list()

for (j in 1:4) {
  HAV_wls_models <- list()
  for (i in 1:length(cluster_vol[[j]])) {
    len.train <- length(cluster_vol.train[[j]][[i]]$volatility)
    weights <- list_HAV_cluster[[j]][[i]]$vol_1 /
               sqrt(cluster_quar[[j]][[i]]$quarticity[5:(len.train - 1)])

    HAV_wls_models[[i]] <- lm(vol ~ vol_1 + mean_vol_5, data = list_HAV_cluster[[j]][[i]],
                              weights = weights)
  }
  list_HAV_wls_cluster[[j]] <- HAV_wls_models
}

list_HAV_wls_cluster[[3]][[1]]
list_HAV_wls_cluster[[2]][[1]]
```

# I am getting an error generated here
# We have a problem here where we're trying to predict the next 5 periods based on less than 5 periods of data 
# Thinking is using the already predicted volatility measures to then reinsert into the equation 
```{r}
# 
# cluster_pred_lm <- list()
# 
# for (j in 1:4) {
#   pred.lm <- list()
#   len.val <- length(cluster_vol.val[[j]][[1]]$volatility)  # Assumes that each cluster has at least one item
#   list_HAV1_cluster <- list()
# 
#   for (i in 1:length(cluster_vol.val[[j]])) {
#     mean.vol <- rep(0, len.val - 5)
#     
#     for (k in 1:5) {
#       if ((k + len.val - 6) <= length(cluster_vol.val[[j]][[i]]$volatility)) {
#         mean.vol <- mean.vol + cluster_vol.val[[j]][[i]]$volatility[k:(k + len.val - 6)] / 5
#       }
#     }
#     
#     list_HAV1_cluster[[i]] <- data.frame(
#       #vol = cluster_vol.val[[j]][[i]]$volatility[-(1:5)], 
#       vol_1 = cluster_vol.val[[j]][[i]]$volatility[5:(len.val - 1)],
#       mean_vol_5 = mean.vol
#     )
# 
#     # Predict using the respective HAV model for this cluster
#     pred.lm[[i]] <- predict(list_HAV_wls_cluster[[j]][[i]], newdata = list_HAV1_cluster[[i]])
#   }
#   
#   cluster_pred_lm[[j]] <- pred.lm
# }
# 
# cluster_vol.val[[4]][[554]]
# list_HAV1_cluster[[i]]

```


# The premise with this code is to be able to predict 5 time periods in the future.
# Previously we would only be predicting one period in the future at a time since we were using the validation data
# This code block proposes an alternative where we predict one time period in the future and then use a rolling window
# to feed each new prediction back into the dataset. 
```{r results = 'hide'}
cluster_pred_lm <- list()

for (j in 1:4) {
  pred.lm <- list()
  latest_obs <- list()
  list_HAV1_cluster <- list()

  for (i in 1:length(cluster_vol.train[[j]])) {
    
    # This will predict 16, 17, 18, 19, 20
    latest_obs[[i]] <- cluster_vol.train[[j]][[i]]$volatility[11:15]
    
    for (t in 1:5) {
        # Compute mean volatility for the last 5 observations
        mean.vol <- sum(latest_obs[[i]])/5

        # Create data frame with updated vol_1 and mean_vol_5
        list_HAV1_cluster[[i]] <- data.frame(
          # vol_1 = cluster_vol.val[[j]][[i]]$volatility[t],
          vol_1 = latest_obs[[i]][5],
          mean_vol_5 = mean.vol
        )

        pred.lm[[t]] <- unname(predict(list_HAV_wls_cluster[[j]][[i]], newdata = list_HAV1_cluster[[i]]))
        
        # Drop the oldest observation and add new predicted value 
        latest_obs[[i]] <- c(latest_obs[[i]][-1], pred.lm[[t]])
    }
    
    #cluster_pred_lm[[j]][[i]] <- latest_obs
    
  }
  cluster_pred_lm[[j]] <- latest_obs
}

cluster_pred_lm[[4]][[2]]

```


```{r warning = FALSE}
cluster_MSE_lm <- list()
cluster_QLIKE_lm <- list()
for (j in 1:4) {
  MSE.lm_1 <- vector()
  QLIKE.lm <- vector()
  for (i in 1:length(cluster_vol.val[[j]])) {
    volatility = cluster_vol.val[[j]][[i]]$volatility
    prediction = cluster_pred_lm[[j]][[i]]
    MSE.lm_1 <- c(MSE.lm_1, mean((volatility - prediction)^2))
    QLIKE.lm <- c(QLIKE.lm, mean(volatility / prediction - 
                                 log(volatility / prediction) - 1))
  }
  cluster_MSE_lm[[j]] <- MSE.lm_1
  cluster_QLIKE_lm[[j]] <- QLIKE.lm
}

par(mfrow = c(2, 2))
for (j in 1:4) {
  boxplot(cluster_MSE_lm[[j]], horizontal = TRUE, main = paste("MSE Plot for cluster", j), ylim = c(0, 0.00001))
}

for (j in 1:4) {
  boxplot(cluster_QLIKE_lm[[j]], horizontal = TRUE, main = paste("QLIKE Plot for cluster", j), ylim = c(0, 2))
}

```


# Computation of HAV-RV and eGARCH weightings
```{r results = 'hide'}

cluster_weights <- list()

for (j in 1:4) {
  standard = Inf
  final_weight_hav = 1
  final_weight_garch = 0
  
  weight_hav = 1
  for (i in 1: 10){
    weight_garch = 1 - weight_hav
    #mix
    mix = vector("list", length = length(cluster_pred_lm[[j]]))
    for (i in 1:length(cluster_pred_lm[[j]])) {
      mix[[i]] = cluster_pred_lm[[j]][[i]]*weight_hav + cluster_rv_pred[[j]][[i]]*weight_garch
    }
    
    MSE.lm <- vector()
    QLIKE.lm <- vector()
    for (i in 1 : length(cluster_vol_val[[j]][[i]]$volatility)) {
      MSE.lm <- c(MSE.lm, mean((cluster_vol_val[[j]][[i]]$volatility - mix[[i]]) ^ 2))
      QLIKE.lm <- c(QLIKE.lm, mean(cluster_vol_val[[j]][[i]]$volatility / mix[[i]] -
                                     log(cluster_vol_val[[j]][[i]]$volatility / mix[[i]]) - 1))
    }

    # mix_mean = mean(na.omit(MSE.lm))
    mix_mean = mean(QLIKE.lm[is.finite(unlist(QLIKE.lm)) & !is.nan(unlist(QLIKE.lm))])
    print(mix_mean < standard)
    if (mix_mean < standard){
      final_weight_hav = weight_hav
      final_weight_garch = weight_garch
      standard = mix_mean
    }
    
    weight_hav = weight_hav - 0.1
  }
  
  cluster_weights[[j]] <- c(final_weight_hav, final_weight_garch)
  
}


cluster_weights[[4]][1]
```

# I have a bad feeling about this code
# There is no difference between 1,0 weights and 'optimized' weights
```{r warning = FALSE}

# Multiple Clusters

cluster_mix <- list()
for (j in 1:4) {
  mix <- list()
  for (i in 1:length(cluster_pred_lm[[j]])){
    mix[[i]] = cluster_pred_lm[[j]][[i]]*cluster_weights[[j]][1] + cluster_rv_pred[[j]][[i]]*cluster_weights[[j]][2]
  } 
  cluster_mix[[j]] <- mix
}

cluster_MSE_mix <- list()
cluster_QLIKE_mix <- list()
for (j in 1:4) {
  MSE.lm_1 <- vector()
  QLIKE.lm <- vector()
  for (i in 1:length(cluster_vol.val[[j]])) {
    MSE.lm_1 <- c(MSE.lm, mean((cluster_vol_val[[j]][[i]]$volatility - cluster_mix[[j]][[i]]) ^ 2))
    QLIKE.lm <- c(QLIKE.lm, mean(cluster_vol_val[[j]][[i]]$volatility / cluster_mix[[j]][[i]] - 
                                 log(cluster_vol_val[[j]][[i]]$volatility / cluster_mix[[j]][[i]]) - 1))
  }
  cluster_MSE_mix[[j]] <- MSE.lm_1
  cluster_QLIKE_mix[[j]] <- QLIKE.lm
}

par(mfrow = c(2, 2))
for (j in 1:4) {
  boxplot(cluster_MSE_mix[[j]], horizontal = TRUE, main = paste("MSE Plot for cluster", j), ylim = c(0, 0.000001))
}

for (j in 1:4) {
  boxplot(cluster_QLIKE_mix[[j]], horizontal = TRUE, main = paste("QLIKE Plot for cluster", j), ylim = c(0, 2))
}

```

```{r}
cluster_weights
```

# Comparative Analysis
```{r warning = FALSE}

# HAV Performance
cluster_MSE_hav <- list()
cluster_QLIKE_hav <- list()
for (j in 1:4) {
  MSE.hav <- vector()
  QLIKE.hav <- vector()
  for (i in 1:length(cluster_vol.val[[j]])) {
    MSE.hav <- c(MSE.hav, mean((cluster_vol_val[[j]][[i]]$volatility - cluster_pred_lm[[j]][[i]]) ^ 2))
    QLIKE.hav <- c(QLIKE.hav, mean(cluster_vol_val[[j]][[i]]$volatility / cluster_pred_lm[[j]][[i]] - 
                                 log(cluster_vol_val[[j]][[i]]$volatility / cluster_pred_lm[[j]][[i]]) - 1))
  }
  cluster_MSE_hav[[j]] <- MSE.hav
  cluster_QLIKE_hav[[j]] <- QLIKE.hav
}

# eGARCH Performance
cluster_MSE_gar <- list()
cluster_QLIKE_gar <- list()
for (j in 1:4) {
  MSE.gar <- vector()
  QLIKE.gar <- vector()
  for (i in 1:length(cluster_vol.val[[j]])) {
    MSE.gar <- c(MSE.gar, mean((cluster_vol_val[[j]][[i]]$volatility - cluster_rv_pred[[j]][[i]]) ^ 2))
    QLIKE.gar <- c(QLIKE.gar, mean(cluster_vol_val[[j]][[i]]$volatility / cluster_rv_pred[[j]][[i]] - 
                                 log(cluster_vol_val[[j]][[i]]$volatility / cluster_rv_pred[[j]][[i]]) - 1))
  }
  cluster_MSE_gar[[j]] <- MSE.gar
  cluster_QLIKE_gar[[j]] <- QLIKE.gar
}

# Mixture Performance
cluster_MSE_mix <- list()
cluster_QLIKE_mix <- list()
for (j in 1:4) {
  MSE.mix <- vector()
  QLIKE.mix <- vector()
  for (i in 1:length(cluster_vol.val[[j]])) {
    MSE.mix <- c(MSE.mix, mean((cluster_vol_val[[j]][[i]]$volatility - cluster_mix[[j]][[i]]) ^ 2))
    QLIKE.mix <- c(QLIKE.mix, mean(cluster_vol_val[[j]][[i]]$volatility / cluster_mix[[j]][[i]] - 
                                 log(cluster_vol_val[[j]][[i]]$volatility / cluster_mix[[j]][[i]]) - 1))
  }
  cluster_MSE_mix[[j]] <- MSE.mix
  cluster_QLIKE_mix[[j]] <- QLIKE.mix
}

# MSE 
MSE_performance <- data.frame(
  Cluster = rep(paste0("Cluster", 1:4), each = 3),
  Model = rep(c("HAV-RV", "eGARCH", "HAV-GAR"), 4),
  MSE = c(cluster_MSE_hav[[1]], cluster_MSE_gar[[1]], cluster_MSE_mix[[1]],
          cluster_MSE_hav[[2]], cluster_MSE_gar[[2]], cluster_MSE_mix[[2]],
          cluster_MSE_hav[[3]], cluster_MSE_gar[[3]], cluster_MSE_mix[[3]],
          cluster_MSE_hav[[4]], cluster_MSE_gar[[4]], cluster_MSE_mix[[4]])
)

# Create data frame for QLIKE
QLIKE_performance <- data.frame(
  Cluster = rep(paste0("Cluster", 1:4), each = 3),
  Model = rep(c("HAV-RV", "eGARCH", "HAV-GAR"), 4),
  QLIKE = c(cluster_QLIKE_hav[[1]], cluster_QLIKE_gar[[1]], cluster_QLIKE_mix[[1]],
            cluster_QLIKE_hav[[2]], cluster_QLIKE_gar[[2]], cluster_QLIKE_mix[[2]],
            cluster_QLIKE_hav[[3]], cluster_QLIKE_gar[[3]], cluster_QLIKE_mix[[3]],
            cluster_QLIKE_hav[[4]], cluster_QLIKE_gar[[4]], cluster_QLIKE_mix[[4]])
)

MSE_performance
```

# Final Boxplots
```{r}

par(mfrow = c(2, 2))

MSE_combined <- rbind(MSE_performance[MSE_performance$Model == "HAV-RV", ],
                      MSE_performance[MSE_performance$Model == "eGARCH", ],
                      MSE_performance[MSE_performance$Model == "HAV-GAR", ])

QLIKE_combined <- rbind(QLIKE_performance[QLIKE_performance$Model == "HAV-RV", ],
                       QLIKE_performance[QLIKE_performance$Model == "eGARCH", ],
                       QLIKE_performance[QLIKE_performance$Model == "HAV-GAR", ])

# Combined MSE plot
for (cluster in 1:4) {
  MSE_combined_filtered <- subset(MSE_combined, Cluster == paste0("Cluster", cluster))
  
  boxplot(MSE ~ Model, data = MSE_combined_filtered, 
          main = paste("Cluster", cluster, "MSE Comparison"),
          ylab = "MSE",
          col = c("skyblue", "lightgreen", "lightpink"),
          border = "black",
          outline = FALSE) 
}

# Combined QLIKE plot
for (cluster in 1:4) {
  QLIKE_combined_filtered <- subset(QLIKE_combined, Cluster == paste0("Cluster", cluster))
  
  boxplot(QLIKE ~ Model, data = QLIKE_combined_filtered, 
          main = paste("Cluster", cluster, "QLIKE Comparison"),
          ylab = "QLIKE",
          col = c("skyblue", "lightgreen", "lightpink"),
          border = "black",
          outline = FALSE)  
}

```



